{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Apache Spark for Everyone - PySpark + Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Markdown](https://en.wikipedia.org/wiki/Markdown) blocks communicate text, images + whatever other useful HTML bits you want to share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like TODO lists:\n",
    "- ~~get [bikes data set](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)~~\n",
    "- ~~import csv~~\n",
    "- ~~do some things with pyspark~~\n",
    "- ~~do some thigns with python~~\n",
    "- show a python vis?\n",
    "- save out file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And code bits:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "```\n",
    "\n",
    "Great Markdown cheatsheet on github [here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set your working directory if you want less pathy things later\n",
    "WORK_DIR = '/Users/amcasari/repos/wwconnect-2016-spark4everyone/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17380"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an RDD from bikes data\n",
    "# sc is an existing SparkContext (initialized when PySpark starts)\n",
    "\n",
    "bikes = sc.textFile(WORK_DIR + \"data/bikes/p*\")\n",
    "bikes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import SQLContext \n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <type 'unicode'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff71d9ce6d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# we'll try to infer the schema from the files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbikes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbikes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/amcasari/sandbox/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amcasari/sandbox/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amcasari/sandbox/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amcasari/sandbox/spark/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <type 'unicode'>"
     ]
    }
   ],
   "source": [
    "# since we are familiar with pandas dataframes, let's convert the RDD to a Spark DataFrame\n",
    "# we'll try to infer the schema from the files\n",
    "\n",
    "bikes_df = sqlContext.createDataFrame(bikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whoops a daisy, let's remove the header, split out the Rows + we can programmatically specify the schema\n",
    "\n",
    "names = bikes.first().replace('\"','')\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the header using subtract\n",
    "bikesHeader = bikes.filter(lambda x: \"instant\" in x)\n",
    "bikesFiltered = bikes.subtract(bikesHeader)\n",
    "bikesFiltered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(instant,StringType,false),\n",
       " StructField(dteday,StringType,false),\n",
       " StructField(season,StringType,false),\n",
       " StructField(yr,StringType,false),\n",
       " StructField(mnth,StringType,false),\n",
       " StructField(hr,StringType,false),\n",
       " StructField(holiday,StringType,false),\n",
       " StructField(weekday,StringType,false),\n",
       " StructField(workingday,StringType,false),\n",
       " StructField(weathersit,StringType,false),\n",
       " StructField(temp,StringType,false),\n",
       " StructField(atemp,StringType,false),\n",
       " StructField(hum,StringType,false),\n",
       " StructField(windspeed,StringType,false),\n",
       " StructField(casual,StringType,false),\n",
       " StructField(registered,StringType,false),\n",
       " StructField(cnt,StringType,false)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# programmatically specify the schema using a StructField\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "fields = [StructField(field_name, StringType(), False) for field_name in names.split(',')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(instant,StringType,false),StructField(dteday,StringType,false),StructField(season,StringType,false),StructField(yr,StringType,false),StructField(mnth,StringType,false),StructField(hr,StringType,false),StructField(holiday,StringType,false),StructField(weekday,StringType,false),StructField(workingday,StringType,false),StructField(weathersit,StringType,false),StructField(temp,StringType,false),StructField(atemp,StringType,false),StructField(hum,StringType,false),StructField(windspeed,StringType,false),StructField(casual,StringType,false),StructField(registered,StringType,false),StructField(cnt,StringType,false)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType(fields)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert each line in the csv to a tuple\n",
    "parts = bikesFiltered.map(lambda l: l.split(\",\"))\n",
    "bikesSplit = parts.map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7], p[8], p[9], p[10],\n",
    "                                 p[11], p[12], p[13], p[14], p[15], p[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD.\n",
    "bikes_df = sqlContext.createDataFrame(bikesSplit, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|    dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|  15086|2012-09-25|     4|  1|   9| 18|      0|      2|         1|         1|0.64|0.6212|0.41|   0.2239|    64|       758|822|\n",
      "|   1060|2011-02-16|     1|  0|   2| 21|      0|      3|         1|         1|0.36|0.3485|0.46|    0.194|     5|        87| 92|\n",
      "|   8138|2011-12-10|     4|  0|  12| 17|      0|      6|         0|         1|0.28|0.2576|0.36|   0.3284|    34|       151|185|\n",
      "|  15068|2012-09-25|     4|  1|   9|  0|      0|      2|         1|         1|0.46|0.4545|0.67|   0.1642|     8|        56| 64|\n",
      "|   6631|2011-10-08|     4|  0|  10| 20|      0|      6|         0|         1|0.52|   0.5|0.77|   0.1045|    78|       124|202|\n",
      "|   4631|2011-07-16|     3|  0|   7| 20|      0|      6|         0|         1|0.72|0.6667|0.51|   0.2239|   108|       188|296|\n",
      "|   2658|2011-04-25|     2|  0|   4| 15|      0|      1|         1|         1|0.74|0.6667|0.51|   0.2239|    50|       125|175|\n",
      "|  14937|2012-09-19|     3|  1|   9| 13|      0|      3|         1|         1| 0.6|0.6212| 0.4|   0.2537|    55|       234|289|\n",
      "|  16174|2012-11-11|     4|  1|  11| 15|      0|      0|         0|         1|0.56|0.5303|0.37|   0.2239|   304|       420|724|\n",
      "|   2036|2011-03-30|     2|  0|   3| 16|      0|      3|         1|         3|0.28|0.2727|0.87|   0.2537|     0|        36| 36|\n",
      "|    721|2011-02-02|     1|  0|   2|  9|      0|      3|         1|         2|0.24|0.2576|0.93|   0.0896|     4|       119|123|\n",
      "|   1006|2011-02-14|     1|  0|   2| 13|      0|      1|         1|         1|0.58|0.5455|0.19|   0.3881|    27|        93|120|\n",
      "|  14599|2012-09-05|     3|  1|   9| 11|      0|      3|         1|         2|0.78|0.7424|0.62|   0.1642|    61|       156|217|\n",
      "|    852|2011-02-07|     1|  0|   2| 22|      0|      1|         1|         1|0.28| 0.303|0.81|   0.0896|     3|        34| 37|\n",
      "|    724|2011-02-02|     1|  0|   2| 12|      0|      3|         1|         2|0.24|0.2273|0.93|   0.2239|     3|        61| 64|\n",
      "|  17280|2012-12-27|     1|  1|  12| 20|      0|      4|         1|         1|0.24|0.2424| 0.6|   0.1642|    12|        79| 91|\n",
      "|   7267|2011-11-04|     4|  0|  11|  9|      0|      5|         1|         2|0.42|0.4242|0.71|   0.4627|    15|       239|254|\n",
      "|   1400|2011-03-03|     1|  0|   3| 14|      0|      4|         1|         1|0.24|0.2576|0.21|   0.1045|    18|        60| 78|\n",
      "|  17097|2012-12-20|     4|  1|  12|  3|      0|      4|         1|         2| 0.3|0.3182| 0.7|   0.0896|     1|         3|  4|\n",
      "|  16246|2012-11-14|     4|  1|  11| 15|      0|      3|         1|         1|0.36|0.3333|0.43|   0.2836|    32|       228|260|\n",
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bikes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instant: string (nullable = false)\n",
      " |-- dteday: string (nullable = false)\n",
      " |-- season: string (nullable = false)\n",
      " |-- yr: string (nullable = false)\n",
      " |-- mnth: string (nullable = false)\n",
      " |-- hr: string (nullable = false)\n",
      " |-- holiday: string (nullable = false)\n",
      " |-- weekday: string (nullable = false)\n",
      " |-- workingday: string (nullable = false)\n",
      " |-- weathersit: string (nullable = false)\n",
      " |-- temp: string (nullable = false)\n",
      " |-- atemp: string (nullable = false)\n",
      " |-- hum: string (nullable = false)\n",
      " |-- windspeed: string (nullable = false)\n",
      " |-- casual: string (nullable = false)\n",
      " |-- registered: string (nullable = false)\n",
      " |-- cnt: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bikes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count() returns a python object of type <type 'int'>\n",
      "number of duplicate rows in the DataFrame: 0\n"
     ]
    }
   ],
   "source": [
    "# now we can look for trends + data quality questions...\n",
    "\n",
    "# total # of rows in the DataFrame\n",
    "num_rows = bikes_df.count()\n",
    "\n",
    "# number of distinct rows in the DataFrame\n",
    "num_distinct = bikes_df.distinct().count()\n",
    "\n",
    "# and we can start to see where pySpark returning python objects can be used locally\n",
    "print \"count() returns a python object of type \" + str(type(num_rows))\n",
    "print \"number of duplicate rows in the DataFrame: \" + str(num_rows - num_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|holiday|count|\n",
      "+-------+-----+\n",
      "|      0|16879|\n",
      "|      1|  500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check out some more df methods\n",
    "bikes_df.groupBy('holiday').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[instant: string, dteday: string, season: string, yr: string, mnth: string, hr: string, holiday: string, weekday: string, workingday: string, weathersit: string, temp: string, atemp: string, hum: string, windspeed: string, casual: string, registered: string, cnt: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's looks at trips in July\n",
    "july_trips = bikes_df.filter(bikes_df['mnth'] == 7)\n",
    "\n",
    "# since we'll be working over the DAG quite a bit, let's persist the RDD in memory\n",
    "july_trips.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1488"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "july_trips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|    dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|   4631|2011-07-16|     3|  0|   7| 20|      0|      6|         0|         1|0.72|0.6667|0.51|   0.2239|   108|       188|296|\n",
      "|  13677|2012-07-29|     3|  1|   7|  1|      0|      0|         0|         1|0.66|0.6061|0.83|   0.1045|    49|       109|158|\n",
      "|  13060|2012-07-03|     3|  1|   7|  8|      0|      2|         1|         1|0.74|0.6818|0.62|   0.0896|    42|       604|646|\n",
      "|  13219|2012-07-09|     3|  1|   7| 23|      0|      1|         1|         2| 0.7|0.6515|0.65|   0.1045|    22|       109|131|\n",
      "|  13233|2012-07-10|     3|  1|   7| 13|      0|      2|         1|         1|0.82|0.7273|0.38|    0.194|    77|       203|280|\n",
      "|   4368|2011-07-05|     3|  0|   7| 21|      0|      2|         1|         1|0.78|0.7424|0.62|   0.2537|    77|       168|245|\n",
      "|  13481|2012-07-20|     3|  1|   7| 21|      0|      5|         1|         3|0.62|0.5758|0.83|   0.3284|    14|       108|122|\n",
      "|   4662|2011-07-18|     3|  0|   7|  3|      0|      1|         1|         1|0.66|0.6061|0.78|    0.194|     3|         4|  7|\n",
      "|   4456|2011-07-09|     3|  0|   7| 13|      0|      6|         0|         1|0.82|0.7424|0.41|   0.2239|   167|       249|416|\n",
      "|  13591|2012-07-25|     3|  1|   7| 11|      0|      3|         1|         1|0.76|0.6667|0.33|        0|    79|       202|281|\n",
      "|  13444|2012-07-19|     3|  1|   7|  8|      0|      4|         1|         1|0.76|0.7121|0.58|   0.2537|    32|       625|657|\n",
      "|  13355|2012-07-15|     3|  1|   7| 15|      0|      0|         0|         1|0.86| 0.803|0.47|   0.1343|   182|       307|489|\n",
      "|  13462|2012-07-20|     3|  1|   7|  2|      0|      5|         1|         2|0.66|0.5909|0.89|   0.0896|     5|        14| 19|\n",
      "|  13500|2012-07-21|     3|  1|   7| 16|      0|      6|         0|         3| 0.6|0.5455|0.88|    0.194|   130|       196|326|\n",
      "|  13282|2012-07-12|     3|  1|   7| 14|      0|      4|         1|         1|0.78|0.6818|0.33|   0.2239|    42|       167|209|\n",
      "|  13077|2012-07-04|     3|  1|   7|  1|      1|      3|         0|         1|0.68|0.6364|0.74|        0|    27|        96|123|\n",
      "|   4953|2011-07-30|     3|  0|   7|  6|      0|      6|         0|         1|0.72|0.6818| 0.7|   0.2985|     6|        18| 24|\n",
      "|   4666|2011-07-18|     3|  0|   7|  7|      0|      1|         1|         1|0.68|0.6364|0.74|   0.2239|    22|       255|277|\n",
      "|   4282|2011-07-02|     3|  0|   7|  7|      0|      6|         0|         1|0.64|0.6061|0.65|        0|    10|        35| 45|\n",
      "|   4319|2011-07-03|     3|  0|   7| 20|      0|      0|         0|         3|0.66|0.6061|0.83|        0|    83|        93|176|\n",
      "+-------+----------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "july_trips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what else would you examine here?\n",
    "# more functions can be found here in documentation (listed in refs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[instant: string, dteday: string, season: string, yr: string, mnth: string, hr: string, holiday: string, weekday: string, workingday: string, weathersit: string, temp: string, atemp: string, hum: string, windspeed: string, casual: string, registered: string, cnt: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we are done working with data, remove from memory\n",
    "july_trips.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:\n",
    "- http://www.nodalpoint.com/spark-data-frames-from-csv-files-handling-headers-column-types/\n",
    "- https://spark.apache.org/docs/1.6.0/api/python/_modules/pyspark/sql/types.html\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
